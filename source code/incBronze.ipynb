{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d0f699-2d11-4d51-b4c7-5c74aa3577f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --------------------------------------\n",
    "# 1) New Bronze Delta Table Name (incremental, sicinc)\n",
    "# --------------------------------------\n",
    "bronze_table_inc = \"sicinc.bronze.enppi_smart_data_inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441eaacd-aa7b-4bce-aaaa-5ba980e00630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"abfss://enppi-row-data@sicdatalakeenppi.dfs.core.windows.net/\"\n",
    "try:\n",
    "    last_processed_timestamp = spark.table(bronze_table_inc)\\\n",
    "        .agg(F.max(F.col(\"ingestion_timestamp\"))).collect()[0][0]\n",
    "except Exception:\n",
    "    last_processed_timestamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6795c7-7ec8-420e-aa99-c99ff4ee8d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if last_processed_timestamp:\n",
    "    bronze_new_df = spark.read.format(\"parquet\").load(bronze_path)\\\n",
    "        .filter(F.col(\"ingestion_timestamp\") > last_processed_timestamp)\n",
    "else:\n",
    "    bronze_new_df = spark.read.format(\"parquet\").load(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ae4c8b-6f93-4a9a-b667-910508309aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(bronze_table_inc):\n",
    "    bronze_new_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table_inc)\n",
    "else:\n",
    "    # Merge incremental\n",
    "    bronze_delta = DeltaTable.forName(spark, bronze_table_inc)\n",
    "    bronze_delta.alias(\"target\").merge(\n",
    "        bronze_new_df.alias(\"source\"),\n",
    "        \"target.sensor_id = source.sensor_id AND target.event_timestamp = source.event_timestamp\"\n",
    "    ).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd469e52-4b59-49f6-a11d-04869d061b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "incBronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}